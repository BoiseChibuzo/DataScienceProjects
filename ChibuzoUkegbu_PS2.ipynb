{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47aaf9b0",
   "metadata": {},
   "source": [
    "# Chibuzo Ukegbu \n",
    "# PS2\n",
    "\n",
    "## Collaborator: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CS534 Homework 2\n",
    "\n",
    "Put your homework in the directory with your name. Please mention in this file the names of any students with whom you collaborated. If you didn't collaborate with anyone, mark your collaborators as \"None.\" Remember, your goal is to communicate. Full credit will be given only to correct solutions which are described clearly. Convoluted and obtuse descriptions will receive low marks. To complete your homework, you may ONLY consult the following material:\n",
    "\n",
    "lecture slides\n",
    "course notes you or others took during lecture.\n",
    "the required text\n",
    "websites that may clarify the concepts covered in the material but do not in any way provide complete solutions to the problems.\n",
    "\n",
    "Please provide an answer to the following question:\n",
    "# Question 1 (15 pts)\n",
    "Implement the fit and predict procedures for the logistic regression (scikit is not allowed) with norm 2 regularization function (and Lambda parameter).\n",
    "Use as the input parameters of the gradient ascent the maximum number of iterations (just a constant e.g 100) and the learning factor (e.g. 0.01).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75caa1e8",
   "metadata": {},
   "source": [
    "## Basic Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707b193e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9d7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisReg:\n",
    "    '''\n",
    "        The maximum number of iteration => num_iter\n",
    "        Learning Factor => lf\n",
    "        Sigmoid Function => sigmoid\n",
    "        Cross-Entropy Loss Function => ce_loss\n",
    "        L2 Regularization lambda => lamd\n",
    "    '''\n",
    "    def __init__(self, num_iter=100, lf=0.01, lamd=0):\n",
    "        self.lf = lf\n",
    "        self.lamd = lamd\n",
    "        self.num_iter = num_iter\n",
    "        self.num_feat = X.shape[1]\n",
    "        self.num_data = X.shape[0]\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        self.w = np.zeros(self.num_feat)\n",
    "        self.b = 0\n",
    "        \n",
    "    def __repr__(self):\n",
    "        op = \"Learning Factor: {}, \\nNumber of iterations: {}, \\nL2 Lambda Parameter: {},\\nWeights: {}, {} \\n\".format(self.lf, self.num_iter, self.lamd, self.w, self.b)\n",
    "        return op\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "       \n",
    "    def ce_loss(self, h, y):\n",
    "        return -(y * np.log(h) + (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def gradient(self, X, y, h):\n",
    "        return (np.dot(X.T, (y - h))) / self.num_data\n",
    "    \n",
    "    def fit(self, X, y, verbose=0):\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.w)\n",
    "            h = self.sigmoid(z)\n",
    "            \n",
    "            self.w += self.lf * self.gradient(X, y, h) + self.lamd * self.w\n",
    "            self.b += (y - h).mean()\n",
    "            \n",
    "            if i % (self.num_iter/10) == 0 and verbose:\n",
    "                print(\"Cost after {} iterations: {}\".format(i, self.ce_loss(h, y)))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.w)).round()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c89c8",
   "metadata": {},
   "source": [
    "## Demo 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bfd1405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Factor: 0.01, \n",
      "Number of iterations: 100, \n",
      "L2 Lambda Parameter: 0.1,\n",
      "Weights: [ 342.89595327 -182.71585587], -25.625464574411662 \n",
      "\n",
      " The accuracy on training is 66.66667\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1\n",
    "\n",
    "model = MyLogisReg(100, 0.01, 0.1)\n",
    "\n",
    "model.fit(X, y, 0)\n",
    "print(model)\n",
    "\n",
    "p = model.predict(X)\n",
    "accuracy = np.sum(p==y)/p.shape[0]*100\n",
    "print(\" The accuracy on training is {:.5f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31515432",
   "metadata": {},
   "source": [
    "## Demo 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20302930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17a070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930545f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dde74ce",
   "metadata": {},
   "source": [
    "# Question 2 (20 pts)\n",
    "Use the iris dataset (just the binary class Iris Setosa vs others), the K-fold cross validation, metrics(accuracy, precision, recall, F1-score) and the logistic regression with L2 regularization.\n",
    "You can use scikit.\n",
    "Please estimate the best parameter C(the inverse of lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b0301",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3788029e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m68\u001b[0m\n\u001b[0;31m    search_range = (search_space[max(best_index-1,0)],\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "X = load_iris().data\n",
    "Y = load_iris().target\n",
    "\n",
    "# Make the class setosa have value of 0, and all other classes have value of 1\n",
    "setosa_index = list(load_iris().target_names).index(\"setosa\")\n",
    "Y[load_iris().target == setosa_index] = 0\n",
    "Y[load_iris().target != setosa_index] = 1\n",
    "\n",
    "def train_logreg(X, Y, C=1):\n",
    "  # Initialize the metrics\n",
    "  metrics = {'accuracy':[], 'precision':[], 'recall':[], 'fscore':[]}\n",
    "\n",
    "  # Train and test on K-fold splits\n",
    "  skf = StratifiedKFold(n_splits=10) \n",
    "  for train_index, test_index in skf.split(X, Y):\n",
    "    # Split the data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Train the logistic regression model\n",
    "    model = LogisticRegression(penalty='l2', C=C)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    Y_pred = model.predict(X_test)\n",
    "    precis, recall, fscore, _ = precision_recall_fscore_support(Y_test, Y_pred, average='micro')\n",
    "    metrics['accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "    metrics['precision'].append(precis)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['fscore'].append(fscore)\n",
    "\n",
    "  # Take the average of the metrics\n",
    "  for k in metrics.keys():\n",
    "    metrics[k] = np.mean(metrics[k])\n",
    "  return metrics\n",
    "\n",
    "# Perform a grid search to find the best C value\n",
    "history = {'C': [], 'fscore': [], 'precision': [], 'recall': [], 'accuracy':[]}\n",
    "grid_search_iterations = 15\n",
    "best_quality = -np.inf\n",
    "search_range = (np.finfo(float).eps, 100000)\n",
    "\n",
    "for _ in range(grid_search_iterations):\n",
    "  # Create a search space\n",
    "    prev_quality = best_quality\n",
    "    search_space = np.linspace(search_range[0], search_range[1], num=10)\n",
    "\n",
    "  # Calculate the quality for each C\n",
    "    search_quality = []\n",
    "for C in search_space:\n",
    "        metrics = train_logreg(X, Y, C)\n",
    "        search_quality.append(metrics['fscore'])\n",
    "\n",
    "        # Add values to history for later plotting\n",
    "        metrics['C'] = C\n",
    "for k in history.keys():\n",
    "    history[k].append(metrics[k])\n",
    "\n",
    "  # Get the best C value and index\n",
    "    best_index = np.argmax(search_quality)\n",
    "    best_quality = search_quality[best_index]\n",
    "    best_C = search_space[best_index]\n",
    "    print((\"In search space {:20} the best C value found was {:.8f},\" + \n",
    "            \" with an F1 Score of {:.2f}\").format(str(list(np.round(search_range, 4))), \n",
    "            best_C, best_quality))\n",
    "\n",
    "  # Calculate a more fine-grain search range\n",
    "  search_range = (search_space[max(best_index-1,0)], \n",
    "                  search_space[min(best_index+1, len(search_space)-1)])\n",
    "\n",
    "print(\"\\n---\\nThe best C value found from the F1-score was the following:\", best_C)\n",
    "print(\"This value has the following metrics:\", train_logreg(X, Y, best_C))\n",
    "\n",
    "# Plot history\n",
    "plt.scatter(history['C'], history['fscore'])\n",
    "plt.xlim(best_C - 0.01, best_C + 0.01)\n",
    "plt.title(\"Search of Best C value for Logistic Regression\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.xlabel(\"Value of C\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221eb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e1b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43246560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
